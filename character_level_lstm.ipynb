{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "character_level_lstm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T01:59:25.386005Z",
          "start_time": "2020-05-22T01:59:18.373566Z"
        },
        "id": "HADi30UP6125",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqxUKehZ6YOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT1WyU336fOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "master = \"https://raw.githubusercontent.com/HimanshuSuman7/nd101_recurrent_neural_network/master/Data/anna_karenina.txt\"\n",
        "req = requests.get(master)\n",
        "TEXT = req.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD7sKy60613D",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:00:22.720047Z",
          "start_time": "2020-05-22T02:00:22.715011Z"
        },
        "id": "glKxaSIE613E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open(\"/content/anna_karenina.txt\", \"rb\") as obj:\n",
        "    # text = obj.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:00:32.672315Z",
          "start_time": "2020-05-22T02:00:32.663314Z"
        },
        "id": "KA4malij613K",
        "colab_type": "code",
        "outputId": "7c269c34-2a4d-45d0-d68f-072fb9a11190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TEXT[:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USBqfTcK613Q",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "Creating two dictionaries:\n",
        "1. int2char -> maps integers to the characters\n",
        "2. char2int -> maps characters to unique integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:08:40.031153Z",
          "start_time": "2020-05-22T02:08:40.000142Z"
        },
        "id": "HKG4dwGT613R",
        "colab_type": "code",
        "outputId": "0d62ed61-5887-4e83-9789-dce8b0138f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "chars = tuple(set(TEXT))\n",
        "print(chars)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(';', ')', 'C', '\\n', 'B', '4', ' ', 'i', '\"', 'K', 'x', 'Z', 'T', 'L', 'a', 'j', '`', 'F', 'y', 'X', 'I', 't', '&', '@', 'c', 'p', 'h', 'A', 'w', 'V', '$', 'z', '_', 'b', '%', 'v', '9', 'J', 'k', 's', 'R', 'P', 'M', 'E', '(', '7', '1', 'u', 'S', 'o', 'q', 'g', 'N', 'l', 'n', 'G', 'Y', '8', '?', 'm', 'Q', '!', '.', '3', '6', '/', '5', ',', '0', 'e', '*', '2', 'f', 'O', \"'\", 'D', 'W', 'r', 'U', 'H', '-', 'd', ':')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:08:51.831728Z",
          "start_time": "2020-05-22T02:08:51.827728Z"
        },
        "id": "sEjgplDB613X",
        "colab_type": "code",
        "outputId": "246685a2-b172-4be4-a2e6-8546a75ea7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "int2char = dict(enumerate(chars))\n",
        "print(int2char)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ';', 1: ')', 2: 'C', 3: '\\n', 4: 'B', 5: '4', 6: ' ', 7: 'i', 8: '\"', 9: 'K', 10: 'x', 11: 'Z', 12: 'T', 13: 'L', 14: 'a', 15: 'j', 16: '`', 17: 'F', 18: 'y', 19: 'X', 20: 'I', 21: 't', 22: '&', 23: '@', 24: 'c', 25: 'p', 26: 'h', 27: 'A', 28: 'w', 29: 'V', 30: '$', 31: 'z', 32: '_', 33: 'b', 34: '%', 35: 'v', 36: '9', 37: 'J', 38: 'k', 39: 's', 40: 'R', 41: 'P', 42: 'M', 43: 'E', 44: '(', 45: '7', 46: '1', 47: 'u', 48: 'S', 49: 'o', 50: 'q', 51: 'g', 52: 'N', 53: 'l', 54: 'n', 55: 'G', 56: 'Y', 57: '8', 58: '?', 59: 'm', 60: 'Q', 61: '!', 62: '.', 63: '3', 64: '6', 65: '/', 66: '5', 67: ',', 68: '0', 69: 'e', 70: '*', 71: '2', 72: 'f', 73: 'O', 74: \"'\", 75: 'D', 76: 'W', 77: 'r', 78: 'U', 79: 'H', 80: '-', 81: 'd', 82: ':'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:09:38.257668Z",
          "start_time": "2020-05-22T02:09:38.253700Z"
        },
        "id": "uJ3ZOBN8613h",
        "colab_type": "code",
        "outputId": "a1af187a-9f4a-4ca1-f837-c646a297d7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "char2int = {ch: count for count, ch in int2char.items()}\n",
        "print(char2int)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{';': 0, ')': 1, 'C': 2, '\\n': 3, 'B': 4, '4': 5, ' ': 6, 'i': 7, '\"': 8, 'K': 9, 'x': 10, 'Z': 11, 'T': 12, 'L': 13, 'a': 14, 'j': 15, '`': 16, 'F': 17, 'y': 18, 'X': 19, 'I': 20, 't': 21, '&': 22, '@': 23, 'c': 24, 'p': 25, 'h': 26, 'A': 27, 'w': 28, 'V': 29, '$': 30, 'z': 31, '_': 32, 'b': 33, '%': 34, 'v': 35, '9': 36, 'J': 37, 'k': 38, 's': 39, 'R': 40, 'P': 41, 'M': 42, 'E': 43, '(': 44, '7': 45, '1': 46, 'u': 47, 'S': 48, 'o': 49, 'q': 50, 'g': 51, 'N': 52, 'l': 53, 'n': 54, 'G': 55, 'Y': 56, '8': 57, '?': 58, 'm': 59, 'Q': 60, '!': 61, '.': 62, '3': 63, '6': 64, '/': 65, '5': 66, ',': 67, '0': 68, 'e': 69, '*': 70, '2': 71, 'f': 72, 'O': 73, \"'\": 74, 'D': 75, 'W': 76, 'r': 77, 'U': 78, 'H': 79, '-': 80, 'd': 81, ':': 82}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:10:11.019914Z",
          "start_time": "2020-05-22T02:10:10.707877Z"
        },
        "id": "PUZxO1j1613n",
        "colab_type": "code",
        "outputId": "a50f5221-fe2d-4429-b3b3-8f9f22fc2e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "encode_text = np.array([char2int[ch] for ch in TEXT])\n",
        "encode_text[:100]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 26, 14, 25, 21, 69, 77,  6, 46,  3,  3,  3, 79, 14, 25, 25, 18,\n",
              "        6, 72, 14, 59,  7, 53,  7, 69, 39,  6, 14, 77, 69,  6, 14, 53, 53,\n",
              "        6, 14, 53,  7, 38, 69,  0,  6, 69, 35, 69, 77, 18,  6, 47, 54, 26,\n",
              "       14, 25, 25, 18,  6, 72, 14, 59,  7, 53, 18,  6,  7, 39,  6, 47, 54,\n",
              "       26, 14, 25, 25, 18,  6,  7, 54,  6,  7, 21, 39,  6, 49, 28, 54,  3,\n",
              "       28, 14, 18, 62,  3,  3, 43, 35, 69, 77, 18, 21, 26,  7, 54])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUGHBOic613v",
        "colab_type": "text"
      },
      "source": [
        "### Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:15:28.529108Z",
          "start_time": "2020-05-22T02:15:28.524108Z"
        },
        "id": "s0Axk00f613w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    # initialize the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    # fill appropriate elements with one\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    # reshape back to original array\n",
        "    one_hot = one_hot.reshape(*arr.shape, n_labels)\n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:16:56.361909Z",
          "start_time": "2020-05-22T02:16:56.355912Z"
        },
        "id": "5iKbABcI6131",
        "colab_type": "code",
        "outputId": "52578131-0921-48d1-9654-d25a8353670e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# test_case\n",
        "\n",
        "test_seq = np.array([1, 2, 3, 4, 5])\n",
        "one_hot = one_hot_encode(test_seq, 10)\n",
        "print(one_hot)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:53:57.304384Z",
          "start_time": "2020-05-22T02:53:57.297391Z"
        },
        "id": "goG6WVxh6136",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    arr = arr.reshape(batch_size, -1)\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T02:57:04.942457Z",
          "start_time": "2020-05-22T02:57:04.936451Z"
        },
        "id": "2qzvSy1z614A",
        "colab_type": "code",
        "outputId": "1ed4a723-5d2a-488b-f6bc-b9b3d0b59759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# test_case\n",
        "\n",
        "batches = get_batches(encode_text, 8, 50)\n",
        "x, y = next(batches)\n",
        "\n",
        "print(\"x = \\n\", x[:10, :10], \"\\n\")\n",
        "print(\"y = \\n\", y[:10, :10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = \n",
            " [[ 2 26 14 25 21 69 77  6 46  3]\n",
            " [39 49 54  6 21 26 14 21  6 14]\n",
            " [69 54 81  6 49 77  6 14  6 72]\n",
            " [39  6 21 26 69  6 24 26  7 69]\n",
            " [ 6 39 14 28  6 26 69 77  6 21]\n",
            " [24 47 39 39  7 49 54  6 14 54]\n",
            " [ 6 27 54 54 14  6 26 14 81  6]\n",
            " [73 33 53 49 54 39 38 18 62  6]] \n",
            "\n",
            "y = \n",
            " [[26 14 25 21 69 77  6 46  3  3]\n",
            " [49 54  6 21 26 14 21  6 14 21]\n",
            " [54 81  6 49 77  6 14  6 72 49]\n",
            " [ 6 21 26 69  6 24 26  7 69 72]\n",
            " [39 14 28  6 26 69 77  6 21 69]\n",
            " [47 39 39  7 49 54  6 14 54 81]\n",
            " [27 54 54 14  6 26 14 81  6 39]\n",
            " [33 53 49 54 39 38 18 62  6  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJv1scNA614H",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T03:03:05.619217Z",
          "start_time": "2020-05-22T03:03:05.616215Z"
        },
        "id": "hKkSkBNm614I",
        "colab_type": "code",
        "outputId": "003fb1b4-8864-4f99-8a60-947aa158c2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "if use_gpu:\n",
        "    print(\"Training on GPU.\")\n",
        "else:\n",
        "    print(\"Using CPU.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T03:44:08.279870Z",
          "start_time": "2020-05-22T03:44:08.269869Z"
        },
        "id": "zQLOKk7L614P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: count for count, ch in self.int2char.items()}\n",
        "        # define LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        # define dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        # define final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        r_out, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_out)\n",
        "        # stack up LSTM outputs\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        if use_gpu:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T03:44:08.617700Z",
          "start_time": "2020-05-22T03:44:08.603699Z"
        },
        "id": "xWPDdxIY614U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    net.train()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    # train on gpu if available\n",
        "    if use_gpu:\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        hidden_state = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            if use_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "            # new copy of hidden_state\n",
        "            hidden_state = tuple([each.data for each in hidden_state])\n",
        "            \n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, hidden_state = net(inputs, hidden_state)\n",
        "            \n",
        "            # calculate loss and perform backpropagation\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # using clip_grad_norm to avoid exploding gradient problem\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # stats\n",
        "            if counter % print_every == 0:\n",
        "                val_hidden = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                \n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    val_hidden = tuple([each.data for each in val_hidden])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if use_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    \n",
        "                    output, val_hidden = net(inputs, val_hidden)\n",
        "                    val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train()\n",
        "                \n",
        "                print(\"epoch : {} / {}, \".format(e+1, epochs),\n",
        "                      \"step : {}, \".format(counter),\n",
        "                      \"loss : {0:.3f}, \".format(loss.item()), \n",
        "                      \"validation_loss : {0:.3f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T03:44:09.468673Z",
          "start_time": "2020-05-22T03:44:09.220222Z"
        },
        "id": "4dv_7z26614Z",
        "colab_type": "code",
        "outputId": "f6702e2f-938e-47bd-a9a3-be46ad74549b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "net"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharRNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-22T04:27:14.607054Z",
          "start_time": "2020-05-22T03:59:11.680108Z"
        },
        "id": "0R1Dt1CI614f",
        "colab_type": "code",
        "outputId": "1a99f6e4-11fe-4e79-bb55-24bfcb60dee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "train_network(net, encode_text, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 1 / 20,  step : 100,  loss : 3.053,  validation_loss : 3.033\n",
            "epoch : 2 / 20,  step : 200,  loss : 2.392,  validation_loss : 2.379\n",
            "epoch : 3 / 20,  step : 300,  loss : 2.144,  validation_loss : 2.106\n",
            "epoch : 3 / 20,  step : 400,  loss : 1.930,  validation_loss : 1.930\n",
            "epoch : 4 / 20,  step : 500,  loss : 1.859,  validation_loss : 1.806\n",
            "epoch : 5 / 20,  step : 600,  loss : 1.726,  validation_loss : 1.713\n",
            "epoch : 6 / 20,  step : 700,  loss : 1.659,  validation_loss : 1.645\n",
            "epoch : 6 / 20,  step : 800,  loss : 1.609,  validation_loss : 1.587\n",
            "epoch : 7 / 20,  step : 900,  loss : 1.542,  validation_loss : 1.539\n",
            "epoch : 8 / 20,  step : 1000,  loss : 1.514,  validation_loss : 1.504\n",
            "epoch : 8 / 20,  step : 1100,  loss : 1.452,  validation_loss : 1.474\n",
            "epoch : 9 / 20,  step : 1200,  loss : 1.428,  validation_loss : 1.448\n",
            "epoch : 10 / 20,  step : 1300,  loss : 1.416,  validation_loss : 1.426\n",
            "epoch : 11 / 20,  step : 1400,  loss : 1.421,  validation_loss : 1.404\n",
            "epoch : 11 / 20,  step : 1500,  loss : 1.364,  validation_loss : 1.391\n",
            "epoch : 12 / 20,  step : 1600,  loss : 1.351,  validation_loss : 1.376\n",
            "epoch : 13 / 20,  step : 1700,  loss : 1.330,  validation_loss : 1.362\n",
            "epoch : 13 / 20,  step : 1800,  loss : 1.331,  validation_loss : 1.352\n",
            "epoch : 14 / 20,  step : 1900,  loss : 1.319,  validation_loss : 1.342\n",
            "epoch : 15 / 20,  step : 2000,  loss : 1.272,  validation_loss : 1.340\n",
            "epoch : 16 / 20,  step : 2100,  loss : 1.282,  validation_loss : 1.327\n",
            "epoch : 16 / 20,  step : 2200,  loss : 1.265,  validation_loss : 1.322\n",
            "epoch : 17 / 20,  step : 2300,  loss : 1.240,  validation_loss : 1.315\n",
            "epoch : 18 / 20,  step : 2400,  loss : 1.274,  validation_loss : 1.313\n",
            "epoch : 18 / 20,  step : 2500,  loss : 1.237,  validation_loss : 1.310\n",
            "epoch : 19 / 20,  step : 2600,  loss : 1.223,  validation_loss : 1.305\n",
            "epoch : 20 / 20,  step : 2700,  loss : 1.236,  validation_loss : 1.293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KW97arF-JX8",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccVhkAy4614l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"rnn_20_epoch.net\"\n",
        "\n",
        "check_point = {\n",
        "    \"n_hidden\": net.n_hidden,\n",
        "    \"n_layers\": net.n_layers,\n",
        "    \"state_dict\": net.state_dict(),\n",
        "    \"tokens\": net.chars\n",
        "}\n",
        "\n",
        "with open(model_name, \"wb\") as f:\n",
        "  torch.save(check_point, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZHinntz-3ES",
        "colab_type": "text"
      },
      "source": [
        "### Top K Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-neUD2o-xDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "  \"\"\"returns: predicted char and hidden_state\"\"\"\n",
        "  # tensor inputs\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "  # check for gpu\n",
        "  if use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "  # detach hidden_state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "  # get character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  if use_gpu:\n",
        "      p = p.cpu()\n",
        "  # get top characters\n",
        "  if top_k is None:\n",
        "      top_ch = np.arange(len(net.chars))\n",
        "  else:\n",
        "      p, top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "  # select the next char\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "  # return encoded value of predicted char and hidden_state\n",
        "  return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8W_XxjRDnUr",
        "colab_type": "text"
      },
      "source": [
        "### Priming and Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXfqw8t3DTdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime=\"The\", top_k=None):\n",
        "  if use_gpu:\n",
        "      net.cuda()\n",
        "  # eval mode\n",
        "  net.eval()\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "      char, h = predict(net, ch, h, top_k=top_k)\n",
        "  chars.append(char)\n",
        "  # rotate chars\n",
        "  for _ in range(size):\n",
        "      char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "      chars.append(char)\n",
        "  # return string\n",
        "  return \"\".join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8RT1B2pEk_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "73f54214-668b-4ae9-c000-8631b0de5b3b"
      },
      "source": [
        "sample(net, 500, prime=\"Anna\", top_k=5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Anna. He went, trying to\\nsay that they would have an offer was in\\nlove. He\\'s always\\ndisagreeable to\\nstand,\\nand what to brought the\\nremoments of another strange, streaghed and with a smile. \"And I can do this instance and tears to make us the correct on their people\\'s fine, and he has not\\ntime to be the mad thim though\\nhe\\'s a conversation.\"\\n\\n\"Well, how\\'s something as the face with the most mistake.\"\\n\\n\"Well, I have nothing to say. In a man\\'s commind in the same time all the meadow, that is, in\\nthat, as'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CML5l7_uFaE_",
        "colab_type": "text"
      },
      "source": [
        "### Loading CheckPoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkUVS6EiEt2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24bd9579-b8b5-475e-c709-ac563fd56e03"
      },
      "source": [
        "with open(\"/content/rnn_20_epoch.net\", \"rb\") as f:\n",
        "    check_point = torch.load(f)\n",
        "\n",
        "load_pt = CharRNN(check_point[\"tokens\"], n_hidden=check_point[\"n_hidden\"], n_layers=check_point[\"n_layers\"])\n",
        "load_pt.load_state_dict(check_point[\"state_dict\"])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEoTaGvGeSl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2c7efb44-d5d8-4a34-c34f-b518214c5443"
      },
      "source": [
        "sample(load_pt, 1000, top_k=5, prime=\"And he said\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'And he said sometimes, and his composide she thought of the same sort of all that had sumperenter with her that what was attacking her husband. He was a strange sang to\\ntheir sensition. Alexey Alexandrovitch was sitting to her as he saw the men shrown in\\nthe side of the reaching things\\nof the same whele on the conversation. \"And it was that she has\\nbeen and with myself. He\\'s that I shall be, to make us, and to\\nbe disputed in that pancion to hurt\\nhis face; there is it to me, and they were suffering all out\\nof the\\nparticular man from their presence. Why are so such meaning of the people, and were says to\\nthe chorce of his mile when he\\'s not always\\nbeen so simple to the country.\"\\n\\n\"If I\\'ll say, they were timilly. His bed was so daughter.\"\\n\\n\"Oh; no,\" answered Stepan Arkadyevitch, and her\\nfamiliar, shrowd, and her face went. \"Yes, tell you all about the meeting of his own step,\" his brother\\nhad been say that in the same stay of his head, the chief short states of the marsh at the dinner to herself to\\nh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiAKkN0ZHWk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}